{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86023,"databundleVersionId":11376393,"sourceType":"competition"},{"sourceId":97669,"databundleVersionId":11615683,"sourceType":"competition"},{"sourceId":166218,"sourceType":"modelInstanceVersion","modelInstanceId":141432,"modelId":164048},{"sourceId":363149,"sourceType":"modelInstanceVersion","modelInstanceId":301527,"modelId":322000}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":17826.18251,"end_time":"2025-05-05T01:52:10.030652","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-04T20:55:03.848142","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bitsandbytes -q","metadata":{"execution":{"iopub.execute_input":"2025-05-04T20:55:08.298364Z","iopub.status.busy":"2025-05-04T20:55:08.297882Z","iopub.status.idle":"2025-05-04T20:56:18.509316Z","shell.execute_reply":"2025-05-04T20:56:18.508621Z"},"papermill":{"duration":70.215876,"end_time":"2025-05-04T20:56:18.510675","exception":false,"start_time":"2025-05-04T20:55:08.294799","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\n\nimport os\nimport platform\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.multiprocessing as mp\nfrom torch.distributed import init_process_group, destroy_process_group\nfrom torch.amp import GradScaler\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, get_cosine_schedule_with_warmup\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import BitsAndBytesConfig\n\nfrom peft import (\n    get_peft_config, \n    get_peft_model, \n    LoraConfig,\n    TaskType,\n    prepare_model_for_kbit_training\n)\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# Constants\nmodel_path = '/kaggle/input/qwen-3/transformers/14b/1'\nnum_folds = 3\nnum_epochs = 3\nbatch_size = 3\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.padding_side = 'left'\n\nclass MathDataset(Dataset):\n    def __init__(self, prompts, targets):\n        self.prompts = prompts\n        self.targets = targets\n\n    def __getitem__(self, idx):\n        return self.prompts[idx], self.targets[idx]\n\n    def __len__(self):\n        return len(self.targets)\n\nclass Net(nn.Module):\n    def __init__(self, model_path, rank):\n        super(Net, self).__init__()\n        self.config = AutoConfig.from_pretrained(model_path)\n\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n        self.backbone = AutoModel.from_pretrained(\n            model_path,\n            use_cache=False,\n            torch_dtype=torch.float16,\n            quantization_config=bnb_config,\n            device_map=rank\n        )\n\n        peft_config = LoraConfig(\n            task_type=TaskType.FEATURE_EXTRACTION,\n            target_modules='all-linear',\n            bias='none',\n            inference_mode=False,\n            r=8,\n            lora_alpha=16,\n            lora_dropout=0.05\n        )\n\n        # self.backbone.gradient_checkpointing_enable()\n\n        # self.backbone = prepare_model_for_kbit_training(self.backbone, use_gradient_checkpointing = True)\n\n        self.backbone = get_peft_model(self.backbone, peft_config)\n\n        \n        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n\n    def forward(self, x):\n        x = self.backbone(**x).last_hidden_state[:, -1, :]\n        return self.head(x)\n\ndef ddp_setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    if platform.system() == 'Windows':\n        os.environ['USE_LIBUV'] = '0'\n        init_process_group(backend='gloo', rank=rank, world_size=world_size)\n    else:\n        init_process_group(backend='nccl', rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\ndef get_optimizer(model, learning_rate=0.0001, diff_lr=0.00001, weight_decay=0.01):\n\n\tno_decay = ['bias', 'LayerNorm.weight']\n\tdifferential_layers = ['backbone']\n\n\toptimizer = torch.optim.AdamW(\n\t\t\t[\n\t\t\t\t{\n\t\t\t\t\t\"params\": [\n\t\t\t\t\t\tparam\n\t\t\t\t\t\tfor name, param in model.named_parameters()\n\t\t\t\t\t\tif (not any(layer in name for layer in differential_layers))\n\t\t\t\t\t\tand (not any(nd in name for nd in no_decay))\n\t\t\t\t\t],\n\t\t\t\t\t\"lr\": learning_rate,\n\t\t\t\t\t\"weight_decay\": weight_decay,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"params\": [\n\t\t\t\t\t\tparam\n\t\t\t\t\t\tfor name, param in model.named_parameters()\n\t\t\t\t\t\tif (not any(layer in name for layer in differential_layers))\n\t\t\t\t\t\tand (any(nd in name for nd in no_decay))\n\t\t\t\t\t],\n\t\t\t\t\t\"lr\": learning_rate,\n\t\t\t\t\t\"weight_decay\": 0,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"params\": [\n\t\t\t\t\t\tparam\n\t\t\t\t\t\tfor name, param in model.named_parameters()\n\t\t\t\t\t\tif (any(layer in name for layer in differential_layers))\n\t\t\t\t\t\tand (not any(nd in name for nd in no_decay))\n\t\t\t\t\t],\n\t\t\t\t\t\"lr\": diff_lr,\n\t\t\t\t\t\"weight_decay\": weight_decay,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"params\": [\n\t\t\t\t\t\tparam\n\t\t\t\t\t\tfor name, param in model.named_parameters()\n\t\t\t\t\t\tif (any(layer in name for layer in differential_layers))\n\t\t\t\t\t\tand (any(nd in name for nd in no_decay))\n\t\t\t\t\t],\n\t\t\t\t\t\"lr\": diff_lr,\n\t\t\t\t\t\"weight_decay\": 0,\n\t\t\t\t},\n\t\t\t],\n\t\t\tlr=learning_rate,\n\t\t\tweight_decay=weight_decay,\n\t)\n\n\treturn optimizer\n\ndef train_model(rank, world_size, num_epochs, fold, train_index, val_index, all_prompts, all_targets):\n    ddp_setup(rank, world_size)\n\n    train_prompts = [all_prompts[i] for i in train_index]\n    val_prompts = [all_prompts[i] for i in val_index]\n    train_targets = [all_targets[i] for i in train_index]\n    val_targets = [all_targets[i] for i in val_index]\n\n    class_weights = 1 / (np.unique(train_targets, return_counts=True)[1] / len(train_targets))\n    class_weights = torch.tensor(class_weights, dtype=torch.half)\n\n    train_dataset = MathDataset(train_prompts, train_targets)\n    val_dataset = MathDataset(val_prompts, val_targets)\n\n    train_sampler = DistributedSampler(train_dataset)\n    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler, pin_memory=True, shuffle=False, drop_last=True)\n    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n\n    model = Net(model_path, rank).to(rank)\n    model = DDP(model, device_ids=[rank])\n\n    optimizer = get_optimizer(model, learning_rate=2e-4, diff_lr=2e-4, weight_decay=0.01)\n        \n    scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,\n                                                num_warmup_steps=0, \n                                                num_training_steps=len(train_loader) * num_epochs)\n    scaler = GradScaler()\n\n    best_f1 = 0.0  # Track best F1\n    for epoch in range(num_epochs):\n        train_loader.sampler.set_epoch(epoch)\n        model.train()\n\n        for batch_prompts, batch_targets in tqdm(train_loader):\n            max_len = max(len(x) for x in tokenizer(batch_prompts).input_ids)\n\n            if max_len > 300:\n                encodings = tokenizer(batch_prompts,\n                  return_tensors='pt', \n                  padding='max_length', \n                  truncation=True,\n                  max_length=300).to(rank)\n            else:\n                encodings = tokenizer(batch_prompts,\n                  return_tensors='pt', \n                  padding='longest').to(rank)            \n            \n            batch_targets = batch_targets.long().to(rank)\n\n            with torch.autocast(device_type='cuda', dtype=torch.float16):\n                logits = model(encodings)\n                loss = F.cross_entropy(logits, batch_targets, weight=class_weights.to(rank))\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n\n        # Validation\n        model.eval()\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch_prompts, batch_targets in tqdm(val_loader, total=len(val_loader)):\n                max_len = max(len(x) for x in tokenizer(batch_prompts).input_ids)\n\n                if max_len > 300:\n                    encodings = tokenizer(batch_prompts,\n                      return_tensors='pt', \n                      padding='max_length', \n                      truncation=True,\n                      max_length=300).to(rank)\n                else:\n                    encodings = tokenizer(batch_prompts,\n                      return_tensors='pt', \n                      padding='longest').to(rank)\n                    \n                with torch.autocast(device_type='cuda', dtype=torch.float16):\n                    \n                    logits = model(encodings)\n                    preds = torch.argmax(logits, dim=1).cpu().tolist()\n    \n                    all_preds.extend(preds)\n                    all_labels.extend(batch_targets)\n\n        f1 = f1_score(all_labels, all_preds, average='micro')\n        print(f'[GPU {rank}] Fold {fold+1} | Epoch {epoch+1}/{num_epochs} | Val F1-micro: {f1:.4f}')\n    \n        if rank == 0 and f1 > best_f1:\n            best_f1 = f1\n            model.eval()\n            model.module.backbone.save_pretrained(f'backbone_fold_{fold}_best')\n            torch.save(model.module.head.state_dict(), f'head_fold_{fold}_best.pt')\n            \n    destroy_process_group()\n\ndef run_ddp(rank, world_size, num_epochs, splits, fold, all_prompts, all_targets):\n    train_index, val_index = splits[fold]\n    train_model(rank, world_size, num_epochs, fold, train_index, val_index, all_prompts, all_targets)\n\nif __name__ == '__main__':\n    print(\"PyTorch version:\", torch.__version__)\n    print(\"CUDA available:\", torch.cuda.is_available())\n    print(\"Number of GPUs available:\", torch.cuda.device_count())\n    \n    torch.manual_seed(1)\n    \n    df = pd.read_csv('/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv')\n    df.columns = ['problem', 'target']\n\n    prompts = [\n        f\"\"\"'<|im_start|>user\nYour task is to classify each Math problem into one of these eight topics using a machine learning or NLP-based approach.\n0: Algebra\n1: Geometry and Trigonometry\n2: Calculus and Analysis\n3: Probability and Statistics\n4: Number Theory\n5: Combinatorics and Discrete Math\n6: Linear Algebra\n7: Abstract Algebra and Topology\n\nYour answer should be an integer that assigns the most appropriate topic category to the given Math problem based on its content and required reasoning.\n\nMath Problem: {p.strip()}\n\nAnswer: \"\"\"\n        for p in df['problem']\n    ]\n\n    targets = df['target'].tolist()\n\n    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n    splits = list(skf.split(prompts, targets))\n\n    world_size = torch.cuda.device_count()\n\n    for fold in range(num_folds):\n        mp.spawn(run_ddp, args=(world_size, num_epochs, splits, fold, prompts, targets), nprocs=world_size)","metadata":{"execution":{"iopub.execute_input":"2025-05-04T20:56:18.544594Z","iopub.status.busy":"2025-05-04T20:56:18.544345Z","iopub.status.idle":"2025-05-04T20:56:18.553322Z","shell.execute_reply":"2025-05-04T20:56:18.552832Z"},"papermill":{"duration":0.027062,"end_time":"2025-05-04T20:56:18.554239","exception":false,"start_time":"2025-05-04T20:56:18.527177","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train.py","metadata":{"execution":{"iopub.execute_input":"2025-05-04T20:56:18.58722Z","iopub.status.busy":"2025-05-04T20:56:18.587031Z","iopub.status.idle":"2025-05-05T01:52:03.803304Z","shell.execute_reply":"2025-05-05T01:52:03.802616Z"},"papermill":{"duration":17745.234265,"end_time":"2025-05-05T01:52:03.804705","exception":false,"start_time":"2025-05-04T20:56:18.57044","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":1.994135,"end_time":"2025-05-05T01:52:07.809826","exception":false,"start_time":"2025-05-05T01:52:05.815691","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}