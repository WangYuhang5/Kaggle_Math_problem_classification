{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86023,"databundleVersionId":11376393,"sourceType":"competition"},{"sourceId":97669,"databundleVersionId":11615683,"sourceType":"competition"},{"sourceId":166218,"sourceType":"modelInstanceVersion","modelInstanceId":141432,"modelId":164048},{"sourceId":166265,"sourceType":"modelInstanceVersion","modelInstanceId":141476,"modelId":164048}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":18109.463411,"end_time":"2025-05-10T12:24:19.582021","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-10T07:22:30.11861","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bitsandbytes -q","metadata":{"execution":{"iopub.status.busy":"2025-05-11T10:20:42.033411Z","iopub.execute_input":"2025-05-11T10:20:42.033576Z","iopub.status.idle":"2025-05-11T10:21:51.657796Z","shell.execute_reply.started":"2025-05-11T10:20:42.033559Z","shell.execute_reply":"2025-05-11T10:21:51.657066Z"},"papermill":{"duration":67.178325,"end_time":"2025-05-10T07:23:41.677762","exception":false,"start_time":"2025-05-10T07:22:34.499437","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile train.py\n\nimport os\nimport platform\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.multiprocessing as mp\nfrom torch.distributed import init_process_group, destroy_process_group\nfrom torch.amp import GradScaler\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, get_cosine_schedule_with_warmup\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import BitsAndBytesConfig\n\n# 禁用 tokenizer 的多线程警告，避免多进程冲突或控制台警告\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n# ========= 模型与训练的常量定义 =========\nmodel_path = '/kaggle/input/qwen2.5/transformers/14b/1'  # Qwen2.5-14B 模型路径\nnum_folds = 3                 # 3折交叉验证\nnum_epochs = 3                # 每折训练3轮\nbatch_size = 2                # 每卡每步处理2条样本\ngrad_accum_steps = 8          # 梯度累计8步，相当于有效 batch size 为 2×8=16\n\n# 加载 Qwen tokenizer，并设置为左侧 padding（适合 decoder-only 模型）\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.padding_side = 'left'\n\n\nclass MathDataset(Dataset):\n    def __init__(self, prompts, targets):\n        self.prompts = prompts      # prompt 是模型的文本输入（如：Classify the topic of this problem: ...）\n        self.targets = targets      # targets 是对应的标签（整数，0~7）\n\n    def __getitem__(self, idx):\n        return self.prompts[idx], self.targets[idx]  # 支持通过索引取出一组样本\n\n    def __len__(self):\n        return len(self.targets)    # 数据集总长度\n\n\nclass Net(nn.Module):\n    def __init__(self, model_path, rank):\n        super(Net, self).__init__()\n        # 读取模型配置（如 hidden_size、层数等）\n        self.config = AutoConfig.from_pretrained(model_path)\n\n\n        # 使用 bitsandbytes 进行 4bit 量化加载配置（节省显存）\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,                       # 启用 4bit 权重量化\n            bnb_4bit_use_double_quant=True,          # 双重量化以进一步压缩\n            bnb_4bit_quant_type=\"nf4\",               # nf4 是非对称量化（更强表达能力）\n            bnb_4bit_compute_dtype=torch.float16     # 使用 float16 进行前向/反向计算\n        )\n\n\n        # 加载 Qwen 模型主干（不带语言模型头），采用量化加载方式\n        self.backbone = AutoModel.from_pretrained(\n            model_path,\n            use_cache=False,                   # 不使用缓存（节省显存）\n            torch_dtype=torch.float16,         # 模型用 float16 推理\n            quantization_config=bnb_config,    # 使用上面定义的 4bit 量化策略\n            device_map=rank                    # 指定 GPU 编号（用于 DDP）\n        )\n\n\n        # 定义 LoRA 配置：对所有线性层注入 r=8 的低秩结构\n        peft_config = LoraConfig(\n            task_type=TaskType.FEATURE_EXTRACTION,  # LoRA 类型为特征抽取（非生成）\n            target_modules='all-linear',            # 应用于所有线性层\n            bias='none',                            # 不引入额外 bias\n            inference_mode=False,                   # 开启训练模式\n            r=8,                                     # LoRA 的秩（子空间维度）\n            lora_alpha=16,                          # 放缩因子\n            lora_dropout=0.05                       # Dropout 防止过拟合\n        )\n\n        # 将主干模型转换为 LoRA 可训练模型\n        self.backbone = get_peft_model(self.backbone, peft_config)\n\n        self.head = nn.Linear(self.config.hidden_size, 8, bias=False)\n\n    def forward(self, x):\n        # 使用 Qwen 模型获取最后一层所有 token 的输出\n        x = self.backbone(**x).last_hidden_state[:, -1, :]\n        # 取最后一个 token 的向量（因为是左 padding，最后一个是输入末尾）\n        return self.head(x)  # 投影到 8 维作为 logits 输出\n\n\ndef ddp_setup(rank, world_size):\n    # 设置主进程的地址和端口（用于各 GPU 之间通信）\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # 如果是 Windows 系统，使用 GLOO 后端（Windows 不支持 NCCL）\n    if platform.system() == 'Windows':\n        os.environ['USE_LIBUV'] = '0'  # 避免与 LibUV 冲突\n        init_process_group(backend='gloo', rank=rank, world_size=world_size)\n    else:\n        # Linux 或其他系统，使用 NCCL（GPU 间通信更快）\n        init_process_group(backend='nccl', rank=rank, world_size=world_size)\n\n    # 指定当前进程使用哪张 GPU（每个 rank 对应一个 GPU）\n    torch.cuda.set_device(rank)\n\n\ndef get_optimizer(model, learning_rate=0.0001, diff_lr=0.00001, weight_decay=0.01):\n    # 不进行权重衰减的参数（如 LayerNorm 和 bias）\n    no_decay = ['bias', 'LayerNorm.weight']\n    \n    # 需要设置“低学习率”的模块（如大模型主干部分）\n    differential_layers = ['backbone']\n\n    optimizer = torch.optim.AdamW(\n        [\n            # 🔹1. 主干之外，且需要 weight decay 的参数\n            {\n                \"params\": [\n                    param for name, param in model.named_parameters()\n                    if (not any(layer in name for layer in differential_layers)) and\n                       (not any(nd in name for nd in no_decay))\n                ],\n                \"lr\": learning_rate,\n                \"weight_decay\": weight_decay,\n            },\n            # 🔹2. 主干之外，但不需要 weight decay 的参数（如 LayerNorm、bias）\n            {\n                \"params\": [\n                    param for name, param in model.named_parameters()\n                    if (not any(layer in name for layer in differential_layers)) and\n                       (any(nd in name for nd in no_decay))\n                ],\n                \"lr\": learning_rate,\n                \"weight_decay\": 0,\n            },\n            # 🔹3. 主干内，且需要 weight decay 的参数（例如 encoder layers）\n            {\n                \"params\": [\n                    param for name, param in model.named_parameters()\n                    if (any(layer in name for layer in differential_layers)) and\n                       (not any(nd in name for nd in no_decay))\n                ],\n                \"lr\": diff_lr,\n                \"weight_decay\": weight_decay,\n            },\n            # 🔹4. 主干内，不需要 weight decay 的参数\n            {\n                \"params\": [\n                    param for name, param in model.named_parameters()\n                    if (any(layer in name for layer in differential_layers)) and\n                       (any(nd in name for nd in no_decay))\n                ],\n                \"lr\": diff_lr,\n                \"weight_decay\": 0,\n            },\n        ],\n        lr=learning_rate,           # 默认学习率（传给 optimizer，用于兼容性）\n        weight_decay=weight_decay, # 默认 weight decay（一般不生效，因为已分组）\n    )\n\n    return optimizer\n\n\ndef train_model(rank, world_size, num_epochs, fold, train_index, val_index, all_prompts, all_targets):\n    ddp_setup(rank, world_size)\n\n    train_prompts = [all_prompts[i] for i in train_index]\n    val_prompts = [all_prompts[i] for i in val_index]\n    train_targets = [all_targets[i] for i in train_index]\n    val_targets = [all_targets[i] for i in val_index]\n\n    class_weights = 1 / (np.unique(train_targets, return_counts=True)[1] / len(train_targets)) #类别越稀有，权重越大（使其 loss 更重要）\n    class_weights = torch.tensor(class_weights, dtype=torch.half)\n\n    train_dataset = MathDataset(train_prompts, train_targets)\n    val_dataset = MathDataset(val_prompts, val_targets)\n\n    train_sampler = DistributedSampler(train_dataset)  #每张 GPU 采样不同数据，保证数据不重复\n    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler, pin_memory=True, shuffle=False, drop_last=True)\n    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size*2, shuffle=False, drop_last=False)\n\n    model = Net(model_path, rank).to(rank)\n    model = DDP(model, device_ids=[rank])\n\n    optimizer = get_optimizer(model, learning_rate=2e-4, diff_lr=2e-4, weight_decay=0.01)\n\n    scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,\n                                                num_warmup_steps=0, \n                                                num_training_steps=(len(train_loader) // grad_accum_steps) * num_epochs)\n    scaler = GradScaler()\n\n    best_f1 = 0.0\n    MAX_LEN = 400\n\n    for epoch in range(num_epochs):\n        train_loader.sampler.set_epoch(epoch)\n        model.train()\n        optimizer.zero_grad()\n\n        for step, (batch_prompts, batch_targets) in enumerate(tqdm(train_loader)):\n            max_len = max(len(x) for x in tokenizer(batch_prompts).input_ids)\n\n            encodings = tokenizer(batch_prompts,\n                                  return_tensors='pt',\n                                  padding='max_length' if max_len > MAX_LEN else 'longest',\n                                  truncation=max_len > MAX_LEN,\n                                  max_length=MAX_LEN).to(rank)\n\n            batch_targets = batch_targets.long().to(rank)\n\n            with torch.autocast(device_type='cuda', dtype=torch.float16):\n                logits = model(encodings)\n                loss = F.cross_entropy(logits, batch_targets, weight=class_weights.to(rank))\n                loss = loss / grad_accum_steps\n\n            scaler.scale(loss).backward()\n\n            if (step + 1) % grad_accum_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()\n\n        # Validation\n        model.eval()\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch_prompts, batch_targets in tqdm(val_loader, total=len(val_loader)):\n                max_len = max(len(x) for x in tokenizer(batch_prompts).input_ids)\n\n                encodings = tokenizer(batch_prompts,\n                                      return_tensors='pt',\n                                      padding='max_length' if max_len > MAX_LEN else 'longest',\n                                      truncation=max_len > MAX_LEN,\n                                      max_length=MAX_LEN).to(rank)\n\n                with torch.autocast(device_type='cuda', dtype=torch.float16):\n                    logits = model(encodings)\n                    preds = torch.argmax(logits, dim=1).cpu().tolist()\n\n                all_preds.extend(preds)\n                all_labels.extend(batch_targets)\n\n        f1 = f1_score(all_labels, all_preds, average='micro')\n        print(f'[GPU {rank}] Fold {fold+1} | Epoch {epoch+1}/{num_epochs} | Val F1-micro: {f1:.4f}')\n\n        if rank == 0 and f1 > best_f1:\n            best_f1 = f1\n            model.eval()\n            model.module.backbone.save_pretrained(f'backbone_fold_{fold}_best')\n            torch.save(model.module.head.state_dict(), f'head_fold_{fold}_best.pt')\n\n    destroy_process_group()\n\ndef run_ddp(rank, world_size, num_epochs, splits, fold, all_prompts, all_targets):\n    train_index, val_index = splits[fold]\n    train_model(rank, world_size, num_epochs, fold, train_index, val_index, all_prompts, all_targets)\n\nif __name__ == '__main__':\n    print(\"PyTorch version:\", torch.__version__)\n    print(\"CUDA available:\", torch.cuda.is_available())\n    print(\"Number of GPUs available:\", torch.cuda.device_count())\n\n    seed = 252\n    torch.manual_seed(seed)\n\n    df = pd.read_csv('/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv')\n    df.columns = ['problem', 'target']\n\n    prompts = [\n        f\"\"\"'<|im_start|>user\nYour task is to classify each Math problem into one of these eight topics using a machine learning or NLP-based approach.\n0: Algebra\n1: Geometry and Trigonometry\n2: Calculus and Analysis\n3: Probability and Statistics\n4: Number Theory\n5: Combinatorics and Discrete Math\n6: Linear Algebra\n7: Abstract Algebra and Topology\n\nYour answer should be an integer that assigns the most appropriate topic category to the given Math problem based on its content and required reasoning.\n\nMath Problem: {tokenizer.decode(tokenizer(p.strip(), return_tensors='pt', padding='max_length', max_length=300, truncation=True).input_ids[0], skip_special_tokens=True)}\n\nAnswer: \"\"\"\n        for p in df['problem']\n    ]\n\n    targets = df['target'].tolist()\n\n    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n    splits = list(skf.split(prompts, targets))\n\n    world_size = torch.cuda.device_count()\n\n    for fold in range(num_folds):\n        mp.spawn(run_ddp, args=(world_size, num_epochs, splits, fold, prompts, targets), nprocs=world_size)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-11T10:23:26.907165Z","iopub.execute_input":"2025-05-11T10:23:26.907846Z","iopub.status.idle":"2025-05-11T10:23:26.918821Z","shell.execute_reply.started":"2025-05-11T10:23:26.907821Z","shell.execute_reply":"2025-05-11T10:23:26.918291Z"},"papermill":{"duration":0.024534,"end_time":"2025-05-10T07:23:41.716783","exception":false,"start_time":"2025-05-10T07:23:41.692249","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Writing train.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!python train.py","metadata":{"execution":{"iopub.status.busy":"2025-05-11T10:23:35.497574Z","iopub.execute_input":"2025-05-11T10:23:35.498215Z","iopub.status.idle":"2025-05-11T15:32:12.585686Z","shell.execute_reply.started":"2025-05-11T10:23:35.498194Z","shell.execute_reply":"2025-05-11T15:32:12.584916Z"},"papermill":{"duration":18030.634037,"end_time":"2025-05-10T12:24:12.364932","exception":false,"start_time":"2025-05-10T07:23:41.730895","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"2025-05-11 10:23:48.906993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746959029.139072     180 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746959029.207550     180 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPyTorch version: 2.5.1+cu124\nCUDA available: True\nNumber of GPUs available: 4\n2025-05-11 10:24:12.469980: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746959052.492010     248 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746959052.498755     248 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-05-11 10:24:20.639444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746959060.660963     315 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746959060.667552     315 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nLoading checkpoint shards:   0%|                          | 0/8 [00:00<?, ?it/s]2025-05-11 10:24:28.933412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746959068.956053     381 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746959068.962922     381 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nLoading checkpoint shards:   0%|                          | 0/8 [00:00<?, ?it/s]2025-05-11 10:24:37.205621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746959077.227748     457 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746959077.234502     457 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nLoading checkpoint shards: 100%|██████████████████| 8/8 [07:23<00:00, 55.39s/it]\nLoading checkpoint shards: 100%|██████████████████| 8/8 [07:23<00:00, 55.40s/it]\n\nLoading checkpoint shards: 100%|██████████████████| 8/8 [07:31<00:00, 56.41s/it]\n100%|█████████████████████████████████████████| 849/849 [19:16<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [19:16<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [19:16<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [19:16<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 850/850 [13:20<00:00,  1.06it/s]\n[GPU 3] Fold 1 | Epoch 1/3 | Val F1-micro: 0.8961\n100%|█████████████████████████████████████████| 850/850 [13:25<00:00,  1.05it/s]\n100%|████████████████████████████████████████▊| 847/850 [13:25<00:03,  1.03s/it][GPU 2] Fold 1 | Epoch 1/3 | Val F1-micro: 0.8961\n100%|█████████████████████████████████████████| 850/850 [13:27<00:00,  1.05it/s]\n[GPU 0] Fold 1 | Epoch 1/3 | Val F1-micro: 0.8961\n100%|█████████████████████████████████████████| 850/850 [13:54<00:00,  1.02it/s]\n[GPU 1] Fold 1 | Epoch 1/3 | Val F1-micro: 0.8961\n100%|█████████████████████████████████████████| 849/849 [19:36<00:00,  1.39s/it]\n100%|█████████████████████████████████████████| 849/849 [19:31<00:00,  1.38s/it]\n100%|█████████████████████████████████████████| 849/849 [19:29<00:00,  1.38s/it]\n100%|█████████████████████████████████████████| 849/849 [19:02<00:00,  1.35s/it]\n100%|█████████████████████████████████████████| 850/850 [13:21<00:00,  1.06it/s]\n 96%|███████████████████████████████████████▎ | 816/850 [13:21<00:34,  1.01s/it][GPU 3] Fold 1 | Epoch 2/3 | Val F1-micro: 0.8990\n100%|█████████████████████████████████████████| 850/850 [13:25<00:00,  1.05it/s]\n[GPU 2] Fold 1 | Epoch 2/3 | Val F1-micro: 0.8990\n100%|█████████████████████████████████████████| 850/850 [13:27<00:00,  1.05it/s]\n[GPU 0] Fold 1 | Epoch 2/3 | Val F1-micro: 0.8990\n100%|█████████████████████████████████████████| 850/850 [13:55<00:00,  1.02it/s]\n[GPU 1] Fold 1 | Epoch 2/3 | Val F1-micro: 0.8990\n100%|█████████████████████████████████████████| 849/849 [19:30<00:00,  1.38s/it]\n100%|█████████████████████████████████████████| 849/849 [19:28<00:00,  1.38s/it]\n100%|█████████████████████████████████████████| 849/849 [19:01<00:00,  1.34s/it]\n100%|█████████████████████████████████████████| 849/849 [19:35<00:00,  1.38s/it]\n100%|█████████████████████████████████████████| 850/850 [13:20<00:00,  1.06it/s]\n[GPU 3] Fold 1 | Epoch 3/3 | Val F1-micro: 0.9096\n100%|█████████████████████████████████████████| 850/850 [13:25<00:00,  1.05it/s]\n[GPU 2] Fold 1 | Epoch 3/3 | Val F1-micro: 0.9096\n100%|█████████████████████████████████████████| 850/850 [13:27<00:00,  1.05it/s]\n[GPU 0] Fold 1 | Epoch 3/3 | Val F1-micro: 0.9096\n100%|█████████████████████████████████████████| 850/850 [13:55<00:00,  1.02it/s]\n[GPU 1] Fold 1 | Epoch 3/3 | Val F1-micro: 0.9096\n2025-05-11 12:11:23.223131: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746965483.245061     680 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746965483.251694     680 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-05-11 12:11:31.518180: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746965491.540081     747 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746965491.546769     747 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nLoading checkpoint shards:   0%|                          | 0/8 [00:00<?, ?it/s]2025-05-11 12:11:39.860100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746965499.881893     816 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746965499.888519     816 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nLoading checkpoint shards:  12%|██▎               | 1/8 [00:03<00:26,  3.72s/it]Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nLoading checkpoint shards:  25%|████▌             | 2/8 [00:09<00:28,  4.72s/it]2025-05-11 12:11:48.320812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746965508.343420     889 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746965508.350257     889 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nLoading checkpoint shards:  38%|██████▊           | 3/8 [00:14<00:25,  5.03s/it]Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nLoading checkpoint shards: 100%|██████████████████| 8/8 [00:36<00:00,  4.60s/it]\nLoading checkpoint shards: 100%|██████████████████| 8/8 [00:36<00:00,  4.57s/it]\nLoading checkpoint shards: 100%|██████████████████| 8/8 [00:36<00:00,  4.58s/it]\nLoading checkpoint shards: 100%|██████████████████| 8/8 [00:36<00:00,  4.57s/it]\n100%|█████████████████████████████████████████| 849/849 [19:17<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [19:17<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [19:17<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [19:18<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [13:17<00:00,  1.06it/s]\n[GPU 3] Fold 2 | Epoch 1/3 | Val F1-micro: 0.8634\n100%|█████████████████████████████████████████| 849/849 [13:20<00:00,  1.06it/s]\n[GPU 2] Fold 2 | Epoch 1/3 | Val F1-micro: 0.8634\n100%|█████████████████████████████████████████| 849/849 [13:26<00:00,  1.05it/s]\n 97%|███████████████████████████████████████▋ | 823/849 [13:26<00:25,  1.04it/s][GPU 0] Fold 2 | Epoch 1/3 | Val F1-micro: 0.8634\n100%|█████████████████████████████████████████| 849/849 [13:50<00:00,  1.02it/s]\n[GPU 1] Fold 2 | Epoch 1/3 | Val F1-micro: 0.8634\n100%|█████████████████████████████████████████| 849/849 [19:36<00:00,  1.39s/it]\n100%|█████████████████████████████████████████| 849/849 [19:29<00:00,  1.38s/it]\n100%|█████████████████████████████████████████| 849/849 [19:05<00:00,  1.35s/it]\n100%|█████████████████████████████████████████| 849/849 [19:40<00:00,  1.39s/it]\n100%|█████████████████████████████████████████| 849/849 [13:16<00:00,  1.07it/s]\n[GPU 3] Fold 2 | Epoch 2/3 | Val F1-micro: 0.9022\n100%|█████████████████████████████████████████| 849/849 [13:19<00:00,  1.06it/s]\n[GPU 2] Fold 2 | Epoch 2/3 | Val F1-micro: 0.9022\n100%|█████████████████████████████████████████| 849/849 [13:25<00:00,  1.05it/s]\n[GPU 0] Fold 2 | Epoch 2/3 | Val F1-micro: 0.9022\n100%|█████████████████████████████████████████| 849/849 [13:51<00:00,  1.02it/s]\n[GPU 1] Fold 2 | Epoch 2/3 | Val F1-micro: 0.9022\n100%|█████████████████████████████████████████| 849/849 [19:37<00:00,  1.39s/it]\n100%|█████████████████████████████████████████| 849/849 [19:35<00:00,  1.38s/it]\n100%|█████████████████████████████████████████| 849/849 [19:28<00:00,  1.38s/it]\n100%|█████████████████████████████████████████| 849/849 [19:03<00:00,  1.35s/it]\n100%|█████████████████████████████████████████| 849/849 [13:16<00:00,  1.07it/s]\n[GPU 3] Fold 2 | Epoch 3/3 | Val F1-micro: 0.9108\n100%|█████████████████████████████████████████| 849/849 [13:19<00:00,  1.06it/s]\n[GPU 2] Fold 2 | Epoch 3/3 | Val F1-micro: 0.9108\n100%|█████████████████████████████████████████| 849/849 [13:25<00:00,  1.05it/s]\n[GPU 0] Fold 2 | Epoch 3/3 | Val F1-micro: 0.9108\n100%|█████████████████████████████████████████| 849/849 [13:50<00:00,  1.02it/s]\n[GPU 1] Fold 2 | Epoch 3/3 | Val F1-micro: 0.9108\n2025-05-11 13:51:40.306602: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746971500.328848    1112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746971500.335884    1112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-05-11 13:51:48.787134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746971508.809300    1179 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746971508.816136    1179 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nLoading checkpoint shards:   0%|                          | 0/8 [00:00<?, ?it/s]2025-05-11 13:51:57.188937: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746971517.210937    1248 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746971517.217682    1248 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nLoading checkpoint shards:  12%|██▎               | 1/8 [00:03<00:25,  3.70s/it]Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nLoading checkpoint shards:  25%|████▌             | 2/8 [00:09<00:28,  4.72s/it]2025-05-11 13:52:05.717862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746971525.739666    1321 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746971525.746308    1321 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nLoading checkpoint shards:  38%|██████▊           | 3/8 [00:14<00:25,  5.04s/it]Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nLoading checkpoint shards: 100%|██████████████████| 8/8 [00:36<00:00,  4.60s/it]\nLoading checkpoint shards: 100%|██████████████████| 8/8 [00:36<00:00,  4.62s/it]\nLoading checkpoint shards: 100%|██████████████████| 8/8 [00:36<00:00,  4.59s/it]\nLoading checkpoint shards: 100%|██████████████████| 8/8 [00:36<00:00,  4.59s/it]\n100%|█████████████████████████████████████████| 849/849 [19:12<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [19:12<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [19:12<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [19:13<00:00,  1.36s/it]\n100%|█████████████████████████████████████████| 849/849 [13:21<00:00,  1.06it/s]\n[GPU 3] Fold 3 | Epoch 1/3 | Val F1-micro: 0.8657\n100%|█████████████████████████████████████████| 849/849 [13:24<00:00,  1.06it/s]\n[GPU 2] Fold 3 | Epoch 1/3 | Val F1-micro: 0.8657\n100%|█████████████████████████████████████████| 849/849 [13:29<00:00,  1.05it/s]\n[GPU 0] Fold 3 | Epoch 1/3 | Val F1-micro: 0.8657\n100%|█████████████████████████████████████████| 849/849 [13:58<00:00,  1.01it/s]\n[GPU 1] Fold 3 | Epoch 1/3 | Val F1-micro: 0.8657\n100%|█████████████████████████████████████████| 849/849 [19:44<00:00,  1.40s/it]\n100%|█████████████████████████████████████████| 849/849 [19:40<00:00,  1.39s/it]\n100%|█████████████████████████████████████████| 849/849 [19:06<00:00,  1.35s/it]\n100%|█████████████████████████████████████████| 849/849 [19:35<00:00,  1.39s/it]\n100%|█████████████████████████████████████████| 849/849 [13:22<00:00,  1.06it/s]\n[GPU 3] Fold 3 | Epoch 2/3 | Val F1-micro: 0.8996\n100%|█████████████████████████████████████████| 849/849 [13:24<00:00,  1.06it/s]\n[GPU 2] Fold 3 | Epoch 2/3 | Val F1-micro: 0.8996\n100%|█████████████████████████████████████████| 849/849 [13:30<00:00,  1.05it/s]\n[GPU 0] Fold 3 | Epoch 2/3 | Val F1-micro: 0.8996\n100%|█████████████████████████████████████████| 849/849 [13:58<00:00,  1.01it/s]\n[GPU 1] Fold 3 | Epoch 2/3 | Val F1-micro: 0.8996\n100%|█████████████████████████████████████████| 849/849 [19:36<00:00,  1.39s/it]\n100%|█████████████████████████████████████████| 849/849 [19:39<00:00,  1.39s/it]\n100%|█████████████████████████████████████████| 849/849 [19:30<00:00,  1.38s/it]\n100%|█████████████████████████████████████████| 849/849 [19:02<00:00,  1.35s/it]\n100%|█████████████████████████████████████████| 849/849 [13:21<00:00,  1.06it/s]\n[GPU 3] Fold 3 | Epoch 3/3 | Val F1-micro: 0.9043\n100%|█████████████████████████████████████████| 849/849 [13:24<00:00,  1.06it/s]\n 96%|███████████████████████████████████████▎ | 813/849 [13:24<00:41,  1.15s/it][GPU 2] Fold 3 | Epoch 3/3 | Val F1-micro: 0.9043\n100%|█████████████████████████████████████████| 849/849 [13:29<00:00,  1.05it/s]\n 96%|███████████████████████████████████████▌ | 819/849 [13:29<00:28,  1.06it/s][GPU 0] Fold 3 | Epoch 3/3 | Val F1-micro: 0.9043\n100%|█████████████████████████████████████████| 849/849 [13:58<00:00,  1.01it/s]\n[GPU 1] Fold 3 | Epoch 3/3 | Val F1-micro: 0.9043\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":2.409747,"end_time":"2025-05-10T12:24:16.958793","exception":false,"start_time":"2025-05-10T12:24:14.549046","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}